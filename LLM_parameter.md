

# ‚≠ê **LLM PARAMETERS (Complete List)**
These parameters control how the LLM **thinks, generates, and behaves**.

# üîµ **1. Generation / Sampling Parameters**

These control **creativity and randomness** of the output.

### **1Ô∏è‚É£ Temperature**

* Controls randomness
* High = creative
* Low = accurate
* Range: 0.0 ‚Äì 2.0

### **2Ô∏è‚É£ Top-p (Nucleus Sampling)**

* Model picks from the smallest set of tokens whose total probability ‚â• p
* Example: top_p = 0.9 ‚Üí choose tokens covering 90% probability

### **3Ô∏è‚É£ Top-k**

* Model picks from only top **k most likely** next tokens
* Example: top_k = 50 ‚Üí choose from 50 best choices

### **4Ô∏è‚É£ Max Tokens / Max Output Tokens**

* Maximum number of tokens the model can generate
* Higher = longer answers

### **5Ô∏è‚É£ Frequency Penalty**

* Reduces repetition of the same words
* High value ‚Üí prevent repeating sentences

### **6Ô∏è‚É£ Presence Penalty**

* Encourages the model to talk about new topics
* Prevents looping

### **7Ô∏è‚É£ Repetition Penalty**

* General punishment for repeated tokens
* Used in many open-source models

### **8Ô∏è‚É£ Stop Sequences**

* Define when the model must stop generating
* Example: stop=["</end>"]

---

# üîµ **2. Behavior / Control Parameters**

These control **how the LLM behaves**.

### **1Ô∏è‚É£ System Prompt / Role**

* Sets the model identity
* "You are a helpful assistant"

### **2Ô∏è‚É£ Seed**

* Controls randomness reproducibility
* Same seed ‚Üí same answer

### **3Ô∏è‚É£ Response Format**

* JSON mode
* Structured output
* OpenAI function calling
* Gemini tool calling

### **4Ô∏è‚É£ Safety Settings**

* Harmful content filters
* Toxicity level
* Corporate guardrails

---

# üîµ **3. Performance Parameters**

These affect **speed, cost, latency**.

### **1Ô∏è‚É£ Context Window**

* Maximum tokens model can handle in prompt + output
* Example: GPT-4o = 128k tokens

### **2Ô∏è‚É£ Batch Size**

* How many inputs are processed in parallel

### **3Ô∏è‚É£ Streaming**

* Token streaming (like ChatGPT typing effect)

---

# üîµ **4. Embedding Parameters**

Used in RAG / vector DB.

### **1Ô∏è‚É£ Embedding Dimensions**

* Length of vector (e.g., 768, 1024, 3072)

### **2Ô∏è‚É£ Chunk Size**

* How much content per chunk before embedding

### **3Ô∏è‚É£ Distance Metric**

* Cosine similarity
* Dot product
* Euclidean

---

# üîµ **5. Fine-Tuning Parameters**

Used when training models.

### **1Ô∏è‚É£ Learning Rate**

### **2Ô∏è‚É£ Number of Epochs**

### **3Ô∏è‚É£ Batch Size**

### **4Ô∏è‚É£ LoRA Rank**

### **5Ô∏è‚É£ Warmup Steps**

---

# üîµ **6. Vendor-Specific Parameters**

Different providers have custom parameters:

### ‚úîÔ∏è **OpenAI (ChatOpenAI)**

* temperature
* top_p
* frequency_penalty
* presence_penalty
* max_tokens
* response_format (JSON mode)
* logprobs

### ‚úîÔ∏è **Gemini (ChatGoogleGenerativeAI)**

* temperature
* top_p
* top_k
* max_output_tokens
* safety_settings
* response_schema

### ‚úîÔ∏è **Claude**

* temperature
* max_tokens
* stop sequences

---

# ‚≠ê **Short**

**LLM parameters control how the model generates text.
Key parameters include temperature, top-p, top-k, max tokens, frequency penalty, and presence penalty.
These define creativity, randomness, repetition control, and length of output.
Different providers like OpenAI, Gemini, and Claude implement slight variations.**

---
